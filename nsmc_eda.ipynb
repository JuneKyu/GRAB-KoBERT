{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import data_util\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "# from transformers import BertForSequnceClassification\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = data_util.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠포스터보고 초딩영화줄오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 솔직히 재미는 없다평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화스파이더맨에서 늙어보이기만 했던 커스틴 던...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149995</th>\n",
       "      <td>6222902</td>\n",
       "      <td>인간이 문제지 소는 뭔죄인가</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149996</th>\n",
       "      <td>8549745</td>\n",
       "      <td>평점이 너무 낮아서</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149997</th>\n",
       "      <td>9311800</td>\n",
       "      <td>이게 뭐요 한국인은 거들먹거리고 필리핀 혼혈은 착하다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149998</th>\n",
       "      <td>2376369</td>\n",
       "      <td>청춘 영화의 최고봉방황과 우울했던 날들의 자화상</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149999</th>\n",
       "      <td>9619869</td>\n",
       "      <td>한국 영화 최초로 수간하는 내용이 담긴 영화</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>145791 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                           document  label\n",
       "0        9976970                                  아 더빙 진짜 짜증나네요 목소리      0\n",
       "1        3819312                         흠포스터보고 초딩영화줄오버연기조차 가볍지 않구나      1\n",
       "2       10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3        9045019                          교도소 이야기구먼 솔직히 재미는 없다평점 조정      0\n",
       "4        6483659  사이몬페그의 익살스런 연기가 돋보였던 영화스파이더맨에서 늙어보이기만 했던 커스틴 던...      1\n",
       "...          ...                                                ...    ...\n",
       "149995   6222902                                    인간이 문제지 소는 뭔죄인가      0\n",
       "149996   8549745                                         평점이 너무 낮아서      1\n",
       "149997   9311800                      이게 뭐요 한국인은 거들먹거리고 필리핀 혼혈은 착하다      0\n",
       "149998   2376369                         청춘 영화의 최고봉방황과 우울했던 날들의 자화상      1\n",
       "149999   9619869                           한국 영화 최초로 수간하는 내용이 담긴 영화      0\n",
       "\n",
       "[145791 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = np.split(train_data.sample(frac=1), [int(0.8*len(train_data))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(tokenizer, sentences, MAX_LEN):\n",
    "\n",
    "    input_ids = []\n",
    "\n",
    "    for i, sent in enumerate(sentences):\n",
    "        if (i + 1) % 1000 == 0:\n",
    "            print(\"tokenizing \" + str(i + 1) + \" out of \" + str(len(sentences)))\n",
    "        encoded_sent = tokenizer.encode(\n",
    "                sent,\n",
    "                add_special_tokens=True,\n",
    "                max_length = 128, # 32 / 64 / 128 / none\n",
    "                pad_to_max_length = True\n",
    "                )\n",
    "        input_ids.append(encoded_sent)\n",
    "\n",
    "    #  pdb.set_trace()\n",
    "\n",
    "    MAX_LEN = max([len(sen) for sen in input_ids])\n",
    "\n",
    "    print(\"padding...\")\n",
    "    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
    "    \n",
    "    attention_masks = []\n",
    "    for sent in input_ids:\n",
    "        att_mask = [int(token_id > 0) for token_id in sent]\n",
    "        attention_masks.append(att_mask)\n",
    "\n",
    "    return input_ids, MAX_LEN, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1349</th>\n",
       "      <td>4546653</td>\n",
       "      <td>이거야 이거 ㅋㅋㅋㅋㅋ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66536</th>\n",
       "      <td>89778</td>\n",
       "      <td>눈물이 끊이지 않는 아름다운 영화다</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84466</th>\n",
       "      <td>10191313</td>\n",
       "      <td>재미 없음 감동 없음 내용 없음 개연성 없음 시간 존나게 아까움 돈도 아까움 별점도...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54310</th>\n",
       "      <td>9726795</td>\n",
       "      <td>프레데터 나오는 부분은 옛날 파워레인저나 백터맨의 크리쳐 나오는 줄 이거 에일리언보...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52936</th>\n",
       "      <td>2708404</td>\n",
       "      <td>액션영화답지않은 지루한전개에 시걸의 얼토당토않은 명언은 더욱 최악 ㅡㅡ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                           document  label\n",
       "1349    4546653                                       이거야 이거 ㅋㅋㅋㅋㅋ      1\n",
       "66536     89778                                눈물이 끊이지 않는 아름다운 영화다      1\n",
       "84466  10191313  재미 없음 감동 없음 내용 없음 개연성 없음 시간 존나게 아까움 돈도 아까움 별점도...      0\n",
       "54310   9726795  프레데터 나오는 부분은 옛날 파워레인저나 백터맨의 크리쳐 나오는 줄 이거 에일리언보...      0\n",
       "52936   2708404            액션영화답지않은 지루한전개에 시걸의 얼토당토않은 명언은 더욱 최악 ㅡㅡ      0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing 1000 out of 116632\n",
      "tokenizing 2000 out of 116632\n",
      "tokenizing 3000 out of 116632\n",
      "tokenizing 4000 out of 116632\n",
      "tokenizing 5000 out of 116632\n",
      "tokenizing 6000 out of 116632\n",
      "tokenizing 7000 out of 116632\n",
      "tokenizing 8000 out of 116632\n",
      "tokenizing 9000 out of 116632\n",
      "tokenizing 10000 out of 116632\n",
      "tokenizing 11000 out of 116632\n",
      "tokenizing 12000 out of 116632\n",
      "tokenizing 13000 out of 116632\n",
      "tokenizing 14000 out of 116632\n",
      "tokenizing 15000 out of 116632\n",
      "tokenizing 16000 out of 116632\n",
      "tokenizing 17000 out of 116632\n",
      "tokenizing 18000 out of 116632\n",
      "tokenizing 19000 out of 116632\n",
      "tokenizing 20000 out of 116632\n",
      "tokenizing 21000 out of 116632\n",
      "tokenizing 22000 out of 116632\n",
      "tokenizing 23000 out of 116632\n",
      "tokenizing 24000 out of 116632\n",
      "tokenizing 25000 out of 116632\n",
      "tokenizing 26000 out of 116632\n",
      "tokenizing 27000 out of 116632\n",
      "tokenizing 28000 out of 116632\n",
      "tokenizing 29000 out of 116632\n",
      "tokenizing 30000 out of 116632\n",
      "tokenizing 31000 out of 116632\n",
      "tokenizing 32000 out of 116632\n",
      "tokenizing 33000 out of 116632\n",
      "tokenizing 34000 out of 116632\n",
      "tokenizing 35000 out of 116632\n",
      "tokenizing 36000 out of 116632\n",
      "tokenizing 37000 out of 116632\n",
      "tokenizing 38000 out of 116632\n",
      "tokenizing 39000 out of 116632\n",
      "tokenizing 40000 out of 116632\n",
      "tokenizing 41000 out of 116632\n",
      "tokenizing 42000 out of 116632\n",
      "tokenizing 43000 out of 116632\n",
      "tokenizing 44000 out of 116632\n",
      "tokenizing 45000 out of 116632\n",
      "tokenizing 46000 out of 116632\n",
      "tokenizing 47000 out of 116632\n",
      "tokenizing 48000 out of 116632\n",
      "tokenizing 49000 out of 116632\n",
      "tokenizing 50000 out of 116632\n",
      "tokenizing 51000 out of 116632\n",
      "tokenizing 52000 out of 116632\n",
      "tokenizing 53000 out of 116632\n",
      "tokenizing 54000 out of 116632\n",
      "tokenizing 55000 out of 116632\n",
      "tokenizing 56000 out of 116632\n",
      "tokenizing 57000 out of 116632\n",
      "tokenizing 58000 out of 116632\n",
      "tokenizing 59000 out of 116632\n",
      "tokenizing 60000 out of 116632\n",
      "tokenizing 61000 out of 116632\n",
      "tokenizing 62000 out of 116632\n",
      "tokenizing 63000 out of 116632\n",
      "tokenizing 64000 out of 116632\n",
      "tokenizing 65000 out of 116632\n",
      "tokenizing 66000 out of 116632\n",
      "tokenizing 67000 out of 116632\n",
      "tokenizing 68000 out of 116632\n",
      "tokenizing 69000 out of 116632\n",
      "tokenizing 70000 out of 116632\n",
      "tokenizing 71000 out of 116632\n",
      "tokenizing 72000 out of 116632\n",
      "tokenizing 73000 out of 116632\n",
      "tokenizing 74000 out of 116632\n",
      "tokenizing 75000 out of 116632\n",
      "tokenizing 76000 out of 116632\n",
      "tokenizing 77000 out of 116632\n",
      "tokenizing 78000 out of 116632\n",
      "tokenizing 79000 out of 116632\n",
      "tokenizing 80000 out of 116632\n",
      "tokenizing 81000 out of 116632\n",
      "tokenizing 82000 out of 116632\n",
      "tokenizing 83000 out of 116632\n",
      "tokenizing 84000 out of 116632\n",
      "tokenizing 85000 out of 116632\n",
      "tokenizing 86000 out of 116632\n",
      "tokenizing 87000 out of 116632\n",
      "tokenizing 88000 out of 116632\n",
      "tokenizing 89000 out of 116632\n",
      "tokenizing 90000 out of 116632\n",
      "tokenizing 91000 out of 116632\n",
      "tokenizing 92000 out of 116632\n",
      "tokenizing 93000 out of 116632\n",
      "tokenizing 94000 out of 116632\n",
      "tokenizing 95000 out of 116632\n",
      "tokenizing 96000 out of 116632\n",
      "tokenizing 97000 out of 116632\n",
      "tokenizing 98000 out of 116632\n",
      "tokenizing 99000 out of 116632\n",
      "tokenizing 100000 out of 116632\n",
      "tokenizing 101000 out of 116632\n",
      "tokenizing 102000 out of 116632\n",
      "tokenizing 103000 out of 116632\n",
      "tokenizing 104000 out of 116632\n",
      "tokenizing 105000 out of 116632\n",
      "tokenizing 106000 out of 116632\n",
      "tokenizing 107000 out of 116632\n",
      "tokenizing 108000 out of 116632\n",
      "tokenizing 109000 out of 116632\n",
      "tokenizing 110000 out of 116632\n",
      "tokenizing 111000 out of 116632\n",
      "tokenizing 112000 out of 116632\n",
      "tokenizing 113000 out of 116632\n",
      "tokenizing 114000 out of 116632\n",
      "tokenizing 115000 out of 116632\n",
      "tokenizing 116000 out of 116632\n",
      "padding...\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 64\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case = True)\n",
    "\n",
    "train_inputs, train_MAX_LEN, train_attn_masks = tokenize(tokenizer, train_data['document'],MAX_LEN)\n",
    "train_labels = train_data['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  101,   100,  1463, 30019, 29991, 30008,   100,   102,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0],\n",
       "       [  101,  1456, 30014, 30021, 29995, 30014, 30022, 29999, 30019,\n",
       "          100,   100,  1463, 30006, 29994, 30017, 30023, 29993, 30006,\n",
       "        29999, 30014, 30021,  1463, 30010, 30025, 30005, 30012, 29993,\n",
       "        30006,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0],\n",
       "       [  101,  1464, 30007, 29995, 30019,   100,  1455, 30006, 30023,\n",
       "        29993, 30011, 30025,   100,  1456, 30007, 29999, 30013, 30025,\n",
       "          100,  1455, 30007, 29999, 30010, 30021, 29997, 30008, 30025,\n",
       "          100,  1461, 30019, 29991, 30006, 30021,  1464, 30011, 30021,\n",
       "        29992, 30006, 29991, 30009,   100,  1457, 30011, 30021, 29993,\n",
       "        30011,   100,  1460, 30010, 30022, 30000, 30008, 30023, 29993,\n",
       "        30011,   100,  1466, 30019, 30022, 29994, 30019, 30025, 30003,\n",
       "        30006, 29999, 30019, 30023, 29999, 30013, 30025, 29999, 30017,\n",
       "        29994, 30011, 29993, 30011,   100,  1463, 30019, 29991, 30008,\n",
       "        30022,  1460, 30011, 29992, 30017, 29994, 30006,   100,   100,\n",
       "         1460, 30014, 30022, 29998, 30006, 30025, 30005, 30006, 30023,\n",
       "          100,   100,  1455, 30017, 30024,  1455, 30017, 30024,  1463,\n",
       "        30010, 30025, 30005, 30012, 29994, 30017, 30022,  1466, 30019,\n",
       "        30022, 29994, 30019, 30025, 30003, 30006, 29999, 30019, 30023,\n",
       "        29999,   102],\n",
       "       [  101,  1468, 30017, 29994, 30009, 29993, 30009, 30003, 30008,\n",
       "         1456, 30006, 29999, 30011, 29992, 30017, 30021,  1460, 30014,\n",
       "        29996, 30014, 30021, 29999, 30017, 30021,   100,  1468, 30006,\n",
       "        29999, 30015, 29994, 30009, 29999, 30019, 30021, 30000, 30008,\n",
       "        29992, 30006,  1460, 30007, 30020, 30003, 30008, 29995, 30007,\n",
       "        30021, 29999, 30018,  1466, 30017, 29994, 30019, 30001, 30010,\n",
       "         1456, 30006, 29999, 30011, 29992, 30017, 30021,  1464, 30014,\n",
       "        30022,  1463, 30019, 29991, 30008,  1463, 30009, 29999, 30019,\n",
       "        30022, 29994, 30019, 29999, 30008, 30021, 29996, 30011, 29993,\n",
       "        30006,   100,  1456, 30006, 29999, 30011, 29991, 30008, 29992,\n",
       "        30006,  1457, 30011, 30025, 29997, 30019, 29993, 30007,  1463,\n",
       "        30010, 30025, 30005, 30012, 29999, 30006, 29992, 30019, 30023,\n",
       "         1463, 30009, 29999, 30019, 30022, 29994, 30019, 29999, 30008,\n",
       "        30021, 29999, 30017, 30021,  1461, 30019, 29993, 30007, 29994,\n",
       "        30017,   102],\n",
       "       [  101,   100,  1464, 30019, 29994, 30014, 30005, 30006, 30021,\n",
       "        30000, 30008, 30021, 29991, 30007, 29999, 30009,  1461, 30019,\n",
       "        29991, 30008, 30022, 29999, 30018,   100,  1459, 30010, 30025,\n",
       "        29999, 30008, 30021, 29999, 30017, 30021,  1457, 30008, 29999,\n",
       "        30014, 30020,   100,   100,   102,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kcc",
   "language": "python",
   "name": "kcc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
