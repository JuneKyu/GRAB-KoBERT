{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import data_util\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "# from transformers import BertForSequnceClassification\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = data_util.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Komoran\n",
    "from textrank import KeywordSummarizer\n",
    "komoran = Komoran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def komoran_tokenizer(sent):\n",
    "    words = komoran.pos(sent, join=True)\n",
    "    words = [w for w in words if ('/NN' in w or '/XR' in w or '/VA' in w or '/VV' in w)]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_string = train_data['document'][57]\n",
    "words = temp_string.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'너무 충격적이엇다 기분을 완전히 푹 꺼지게 하는 느낌 활력이라고는 하나도 없는 너무나도 무거운지독하고 차갑고 무자비하다 그저 일본인들의 상상력은 정말 대단한거 같다는 생각이 든다'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['document'][57]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['너무',\n",
       " '충격적이엇다',\n",
       " '기분을',\n",
       " '완전히',\n",
       " '푹',\n",
       " '꺼지게',\n",
       " '하는',\n",
       " '느낌',\n",
       " '활력이라고는',\n",
       " '하나도',\n",
       " '없는',\n",
       " '너무나도',\n",
       " '무거운지독하고',\n",
       " '차갑고',\n",
       " '무자비하다',\n",
       " '그저',\n",
       " '일본인들의',\n",
       " '상상력은',\n",
       " '정말',\n",
       " '대단한거',\n",
       " '같다는',\n",
       " '생각이',\n",
       " '든다']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('독하/VA', 1.0), ('무겁/VA', 1.0), ('거/NNB', 1.0), ('대단/XR', 1.0), ('기분/NNG', 0.15000000000000002), ('꺼지/VV', 0.15000000000000002), ('하/VV', 0.15000000000000002), ('느낌/NNG', 0.15000000000000002), ('활력/NNG', 0.15000000000000002), ('없/VA', 0.15000000000000002), ('들/VV', 0.15000000000000002), ('생각/NNG', 0.15000000000000002), ('차갑/VA', 0.15000000000000002), ('자비/NNG', 0.15000000000000002), ('일본인/NNG', 0.15000000000000002), ('상상력/NNP', 0.15000000000000002), ('같/VA', 0.15000000000000002), ('충격/NNG', 0.15000000000000002)]\n"
     ]
    }
   ],
   "source": [
    "from textrank import KeywordSummarizer\n",
    "\n",
    "summarizer = KeywordSummarizer(tokenize=komoran_tokenizer, min_count=1, min_cooccurrence=1)\n",
    "summa_words = summarizer.summarize(words, topk=30)\n",
    "print(summa_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = summa_words[0][0]\n",
    "# temp = []\n",
    "# for \n",
    "temp = [summa_words[i][0] for i in range(len(summa_words))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['독하/VA',\n",
       " '무겁/VA',\n",
       " '거/NNB',\n",
       " '대단/XR',\n",
       " '기분/NNG',\n",
       " '꺼지/VV',\n",
       " '하/VV',\n",
       " '느낌/NNG',\n",
       " '활력/NNG',\n",
       " '없/VA',\n",
       " '들/VV',\n",
       " '생각/NNG',\n",
       " '차갑/VA',\n",
       " '자비/NNG',\n",
       " '일본인/NNG',\n",
       " '상상력/NNP',\n",
       " '같/VA',\n",
       " '충격/NNG']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'사이몬페그의 익살스런 연기가 돋보였던 영화스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "였\n",
      "0.15000000000000002\n",
      "가\n",
      "0.15000000000000002\n",
      "기\n",
      "0.15000000000000002\n",
      "던\n",
      "0.15000000000000002\n",
      "보\n",
      "0.15000000000000002\n",
      "스\n",
      "0.15000000000000002\n",
      "이\n",
      "0.15000000000000002\n"
     ]
    }
   ],
   "source": [
    "from textrank import KeywordSummarizer\n",
    "\n",
    "docs = ['list of str form', 'sentence list']\n",
    "\n",
    "keyword_extractor = KeywordSummarizer(\n",
    "    tokenize = lambda x:x.split(),      # YOUR TOKENIZER\n",
    "    window = -1,\n",
    "    verbose = False\n",
    ")\n",
    "\n",
    "sents = temp_string\n",
    "\n",
    "keywords = keyword_extractor.summarize(sents, topk=30)\n",
    "for word, rank in keywords:\n",
    "    print(word)\n",
    "    print(rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = np.split(train_data.sample(frac=1), [int(0.8*len(train_data))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(tokenizer, sentences, MAX_LEN):\n",
    "\n",
    "    input_ids = []\n",
    "\n",
    "    for i, sent in enumerate(sentences):\n",
    "        if (i + 1) % 1000 == 0:\n",
    "            print(\"tokenizing \" + str(i + 1) + \" out of \" + str(len(sentences)))\n",
    "        encoded_sent = tokenizer.encode(\n",
    "                sent,\n",
    "                add_special_tokens=True,\n",
    "                max_length = 128, # 32 / 64 / 128 / none\n",
    "                pad_to_max_length = True\n",
    "                )\n",
    "        input_ids.append(encoded_sent)\n",
    "\n",
    "    #  pdb.set_trace()\n",
    "\n",
    "    MAX_LEN = max([len(sen) for sen in input_ids])\n",
    "\n",
    "    print(\"padding...\")\n",
    "    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
    "    \n",
    "    attention_masks = []\n",
    "    for sent in input_ids:\n",
    "        att_mask = [int(token_id > 0) for token_id in sent]\n",
    "        attention_masks.append(att_mask)\n",
    "\n",
    "    return input_ids, MAX_LEN, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>9063648</td>\n",
       "      <td>영화가 사람의 영혼을 어루만져 줄 수도 있군요 거친 세상사를 잠시 잊고 동화같은 영...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>8272095</td>\n",
       "      <td>야 세르게이 작은고추의 매운맛을 보여주마 포퐁저그 콩진호가 간다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>2345905</td>\n",
       "      <td>이렇게 가슴시리게 본 드라마가 또 있을까 감동 그 자체</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>7865630</td>\n",
       "      <td>난또 저 꼬마애가 무슨 원한이 깊길래 했더니  그냥 혼자 나대다 걸 어쩌라고</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>7207064</td>\n",
       "      <td>재미있어요</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>5719655</td>\n",
       "      <td>전 좋아요</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1651126</td>\n",
       "      <td>최고</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>7246040</td>\n",
       "      <td>너무 충격적이엇다 기분을 완전히 푹 꺼지게 하는 느낌 활력이라고는 하나도 없는 너무...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>717775</td>\n",
       "      <td>심심한영화</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>8317483</td>\n",
       "      <td>백봉기 언제나오나요</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "50  9063648  영화가 사람의 영혼을 어루만져 줄 수도 있군요 거친 세상사를 잠시 잊고 동화같은 영...      1\n",
       "51  8272095                야 세르게이 작은고추의 매운맛을 보여주마 포퐁저그 콩진호가 간다      0\n",
       "52  2345905                     이렇게 가슴시리게 본 드라마가 또 있을까 감동 그 자체      1\n",
       "53  7865630         난또 저 꼬마애가 무슨 원한이 깊길래 했더니  그냥 혼자 나대다 걸 어쩌라고      0\n",
       "54  7207064                                              재미있어요      1\n",
       "55  5719655                                              전 좋아요      1\n",
       "56  1651126                                                 최고      0\n",
       "57  7246040  너무 충격적이엇다 기분을 완전히 푹 꺼지게 하는 느낌 활력이라고는 하나도 없는 너무...      1\n",
       "58   717775                                              심심한영화      0\n",
       "59  8317483                                         백봉기 언제나오나요      1"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  50 - posi\n",
    "#  57 - nega\n",
    "train_data[50:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing 1000 out of 116632\n",
      "tokenizing 2000 out of 116632\n",
      "tokenizing 3000 out of 116632\n",
      "tokenizing 4000 out of 116632\n",
      "tokenizing 5000 out of 116632\n",
      "tokenizing 6000 out of 116632\n",
      "tokenizing 7000 out of 116632\n",
      "tokenizing 8000 out of 116632\n",
      "tokenizing 9000 out of 116632\n",
      "tokenizing 10000 out of 116632\n",
      "tokenizing 11000 out of 116632\n",
      "tokenizing 12000 out of 116632\n",
      "tokenizing 13000 out of 116632\n",
      "tokenizing 14000 out of 116632\n",
      "tokenizing 15000 out of 116632\n",
      "tokenizing 16000 out of 116632\n",
      "tokenizing 17000 out of 116632\n",
      "tokenizing 18000 out of 116632\n",
      "tokenizing 19000 out of 116632\n",
      "tokenizing 20000 out of 116632\n",
      "tokenizing 21000 out of 116632\n",
      "tokenizing 22000 out of 116632\n",
      "tokenizing 23000 out of 116632\n",
      "tokenizing 24000 out of 116632\n",
      "tokenizing 25000 out of 116632\n",
      "tokenizing 26000 out of 116632\n",
      "tokenizing 27000 out of 116632\n",
      "tokenizing 28000 out of 116632\n",
      "tokenizing 29000 out of 116632\n",
      "tokenizing 30000 out of 116632\n",
      "tokenizing 31000 out of 116632\n",
      "tokenizing 32000 out of 116632\n",
      "tokenizing 33000 out of 116632\n",
      "tokenizing 34000 out of 116632\n",
      "tokenizing 35000 out of 116632\n",
      "tokenizing 36000 out of 116632\n",
      "tokenizing 37000 out of 116632\n",
      "tokenizing 38000 out of 116632\n",
      "tokenizing 39000 out of 116632\n",
      "tokenizing 40000 out of 116632\n",
      "tokenizing 41000 out of 116632\n",
      "tokenizing 42000 out of 116632\n",
      "tokenizing 43000 out of 116632\n",
      "tokenizing 44000 out of 116632\n",
      "tokenizing 45000 out of 116632\n",
      "tokenizing 46000 out of 116632\n",
      "tokenizing 47000 out of 116632\n",
      "tokenizing 48000 out of 116632\n",
      "tokenizing 49000 out of 116632\n",
      "tokenizing 50000 out of 116632\n",
      "tokenizing 51000 out of 116632\n",
      "tokenizing 52000 out of 116632\n",
      "tokenizing 53000 out of 116632\n",
      "tokenizing 54000 out of 116632\n",
      "tokenizing 55000 out of 116632\n",
      "tokenizing 56000 out of 116632\n",
      "tokenizing 57000 out of 116632\n",
      "tokenizing 58000 out of 116632\n",
      "tokenizing 59000 out of 116632\n",
      "tokenizing 60000 out of 116632\n",
      "tokenizing 61000 out of 116632\n",
      "tokenizing 62000 out of 116632\n",
      "tokenizing 63000 out of 116632\n",
      "tokenizing 64000 out of 116632\n",
      "tokenizing 65000 out of 116632\n",
      "tokenizing 66000 out of 116632\n",
      "tokenizing 67000 out of 116632\n",
      "tokenizing 68000 out of 116632\n",
      "tokenizing 69000 out of 116632\n",
      "tokenizing 70000 out of 116632\n",
      "tokenizing 71000 out of 116632\n",
      "tokenizing 72000 out of 116632\n",
      "tokenizing 73000 out of 116632\n",
      "tokenizing 74000 out of 116632\n",
      "tokenizing 75000 out of 116632\n",
      "tokenizing 76000 out of 116632\n",
      "tokenizing 77000 out of 116632\n",
      "tokenizing 78000 out of 116632\n",
      "tokenizing 79000 out of 116632\n",
      "tokenizing 80000 out of 116632\n",
      "tokenizing 81000 out of 116632\n",
      "tokenizing 82000 out of 116632\n",
      "tokenizing 83000 out of 116632\n",
      "tokenizing 84000 out of 116632\n",
      "tokenizing 85000 out of 116632\n",
      "tokenizing 86000 out of 116632\n",
      "tokenizing 87000 out of 116632\n",
      "tokenizing 88000 out of 116632\n",
      "tokenizing 89000 out of 116632\n",
      "tokenizing 90000 out of 116632\n",
      "tokenizing 91000 out of 116632\n",
      "tokenizing 92000 out of 116632\n",
      "tokenizing 93000 out of 116632\n",
      "tokenizing 94000 out of 116632\n",
      "tokenizing 95000 out of 116632\n",
      "tokenizing 96000 out of 116632\n",
      "tokenizing 97000 out of 116632\n",
      "tokenizing 98000 out of 116632\n",
      "tokenizing 99000 out of 116632\n",
      "tokenizing 100000 out of 116632\n",
      "tokenizing 101000 out of 116632\n",
      "tokenizing 102000 out of 116632\n",
      "tokenizing 103000 out of 116632\n",
      "tokenizing 104000 out of 116632\n",
      "tokenizing 105000 out of 116632\n",
      "tokenizing 106000 out of 116632\n",
      "tokenizing 107000 out of 116632\n",
      "tokenizing 108000 out of 116632\n",
      "tokenizing 109000 out of 116632\n",
      "tokenizing 110000 out of 116632\n",
      "tokenizing 111000 out of 116632\n",
      "tokenizing 112000 out of 116632\n",
      "tokenizing 113000 out of 116632\n",
      "tokenizing 114000 out of 116632\n",
      "tokenizing 115000 out of 116632\n",
      "tokenizing 116000 out of 116632\n",
      "padding...\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 64\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case = True)\n",
    "\n",
    "train_inputs, train_MAX_LEN, train_attn_masks = tokenize(tokenizer, train_data['document'],MAX_LEN)\n",
    "train_labels = train_data['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  101,   100,  1463, 30019, 29991, 30008,   100,   102,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0],\n",
       "       [  101,  1456, 30014, 30021, 29995, 30014, 30022, 29999, 30019,\n",
       "          100,   100,  1463, 30006, 29994, 30017, 30023, 29993, 30006,\n",
       "        29999, 30014, 30021,  1463, 30010, 30025, 30005, 30012, 29993,\n",
       "        30006,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0],\n",
       "       [  101,  1464, 30007, 29995, 30019,   100,  1455, 30006, 30023,\n",
       "        29993, 30011, 30025,   100,  1456, 30007, 29999, 30013, 30025,\n",
       "          100,  1455, 30007, 29999, 30010, 30021, 29997, 30008, 30025,\n",
       "          100,  1461, 30019, 29991, 30006, 30021,  1464, 30011, 30021,\n",
       "        29992, 30006, 29991, 30009,   100,  1457, 30011, 30021, 29993,\n",
       "        30011,   100,  1460, 30010, 30022, 30000, 30008, 30023, 29993,\n",
       "        30011,   100,  1466, 30019, 30022, 29994, 30019, 30025, 30003,\n",
       "        30006, 29999, 30019, 30023, 29999, 30013, 30025, 29999, 30017,\n",
       "        29994, 30011, 29993, 30011,   100,  1463, 30019, 29991, 30008,\n",
       "        30022,  1460, 30011, 29992, 30017, 29994, 30006,   100,   100,\n",
       "         1460, 30014, 30022, 29998, 30006, 30025, 30005, 30006, 30023,\n",
       "          100,   100,  1455, 30017, 30024,  1455, 30017, 30024,  1463,\n",
       "        30010, 30025, 30005, 30012, 29994, 30017, 30022,  1466, 30019,\n",
       "        30022, 29994, 30019, 30025, 30003, 30006, 29999, 30019, 30023,\n",
       "        29999,   102],\n",
       "       [  101,  1468, 30017, 29994, 30009, 29993, 30009, 30003, 30008,\n",
       "         1456, 30006, 29999, 30011, 29992, 30017, 30021,  1460, 30014,\n",
       "        29996, 30014, 30021, 29999, 30017, 30021,   100,  1468, 30006,\n",
       "        29999, 30015, 29994, 30009, 29999, 30019, 30021, 30000, 30008,\n",
       "        29992, 30006,  1460, 30007, 30020, 30003, 30008, 29995, 30007,\n",
       "        30021, 29999, 30018,  1466, 30017, 29994, 30019, 30001, 30010,\n",
       "         1456, 30006, 29999, 30011, 29992, 30017, 30021,  1464, 30014,\n",
       "        30022,  1463, 30019, 29991, 30008,  1463, 30009, 29999, 30019,\n",
       "        30022, 29994, 30019, 29999, 30008, 30021, 29996, 30011, 29993,\n",
       "        30006,   100,  1456, 30006, 29999, 30011, 29991, 30008, 29992,\n",
       "        30006,  1457, 30011, 30025, 29997, 30019, 29993, 30007,  1463,\n",
       "        30010, 30025, 30005, 30012, 29999, 30006, 29992, 30019, 30023,\n",
       "         1463, 30009, 29999, 30019, 30022, 29994, 30019, 29999, 30008,\n",
       "        30021, 29999, 30017, 30021,  1461, 30019, 29993, 30007, 29994,\n",
       "        30017,   102],\n",
       "       [  101,   100,  1464, 30019, 29994, 30014, 30005, 30006, 30021,\n",
       "        30000, 30008, 30021, 29991, 30007, 29999, 30009,  1461, 30019,\n",
       "        29991, 30008, 30022, 29999, 30018,   100,  1459, 30010, 30025,\n",
       "        29999, 30008, 30021, 29999, 30017, 30021,  1457, 30008, 29999,\n",
       "        30014, 30020,   100,   100,   102,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['사이몬페그의',\n",
    " '익살스런',\n",
    " '연기가',\n",
    " '돋보였던',\n",
    " '영화스파이더맨에서',\n",
    " '늙어보이기만',\n",
    " '했던',\n",
    " '커스틴',\n",
    " '던스트가',\n",
    " '너무나도',\n",
    " '이뻐보였다']\n",
    "\n",
    "[('티/NNG', 1.0),\n",
    " ('커스/NNP', 1.0),\n",
    " ('보이/VV', 1.0),\n",
    " ('늙/VV', 1.0),\n",
    " ('스파이더맨/NNP', 1.0),\n",
    " ('영화/NNG', 1.0),\n",
    " ('하/VV', 0.15000000000000002),\n",
    " ('돋보이/VV', 0.15000000000000002),\n",
    " ('연기/NNG', 0.15000000000000002),\n",
    " ('익살/NNG', 0.15000000000000002)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kcc",
   "language": "python",
   "name": "kcc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
